import json
import random
import asyncio
import shutil
import os
import sys
import time
import itertools
from hashlib import sha256

from project.coordinator.data_classes import Peer, decode_peers
from project.messages.body import MsgType, GCHNK_body, APEER_body, ErrorCode, ACHNK_body, REPRT_body
from project.messages.pack import pack, unpack
import threading

global IP_ADDR

CHUNK_SIZE = 1024

MAX_CONNECTIONS = 2

RETRY_SECONDS = 5

REPORTING_INTERVAL_SECONDS = 3


async def _query_coordinator_for_file_peers(file_hash: str) -> list[Peer]:
    """
    Query coordinator for list of peers which have the given file
    """
    reader, writer = await asyncio.open_connection("10.5.0.10", 8000)
    try:
        apeer_msg = pack(APEER_body(msg_type=MsgType.APEER.value, file_hash=str.encode(file_hash)))
        writer.write(apeer_msg)
        await writer.drain()
        msg_type = await reader.readexactly(n=1)
    finally:
        writer.close()

    # check mgs type
    if int.from_bytes(msg_type, byteorder="little", signed=False) == MsgType.ERROR.value:
        body = await reader.readexactly(n=3)
        error_msg = unpack(msg_type + body, msg_type=MsgType.ERROR)
        if error_msg.error_code == ErrorCode.NO_FILE_FOUND.value:
            raise ValueError("File not found")
        else:
            raise ValueError("Error downloading file")
    # unpack the message
    prefix = msg_type + await reader.readexactly(n=43)
    num_peers = int.from_bytes(prefix[-4:], byteorder="little", signed=False)
    peers_bytes = await reader.readexactly(n=num_peers * 8)
    peers_response = unpack(prefix + peers_bytes, MsgType.PEERS)
    return [
        peer
        for peer in decode_peers(peers_response.peers)
        if peer.address != IP_ADDR and not peer.address.startswith("10.5.0.2")
    ]  # FIXME workaround to disable connecting to other peers until we have proper seeding


async def _download_chunk(
    reader: asyncio.StreamReader, writer: asyncio.StreamWriter, file_hash: str, chunk_num: int
) -> bytes:
    """
    Download a given chunk using provided reader and writer and return it
    """
    file_hash_bytes = str.encode(file_hash)
    gchnk_bytes = pack(GCHNK_body(msg_type=MsgType.GCHNK.value, file_hash=file_hash_bytes, chunk_num=chunk_num))
    writer.write(gchnk_bytes)
    await writer.drain()
    prefix = await reader.readexactly(n=72 + CHUNK_SIZE)
    unpacked = unpack(data=prefix, msg_type=MsgType.SCHNK)
    assert unpacked.msg_type == MsgType.SCHNK.value
    assert unpacked.chunk_num == chunk_num
    assert unpacked.file_hash == file_hash_bytes

    return unpacked.content


async def download_file(fileinfo: dict, out_name: str):
    """
    Download a file and save it under `out_name`. Use `fileinfo` generated by `create_fileinfo.py`.
    """

    partial_file = f"{out_name}.partial"

    size = fileinfo["file_size"]
    file_hash = fileinfo["file_hash"]
    chunk_hashes = fileinfo["chunk_hashes"]
    num_chunks = fileinfo["num_chunks"]

    with open(partial_file, "wb") as fp:
        fp.truncate(size)

    retry_queue = asyncio.Queue()

    sem = asyncio.Semaphore(MAX_CONNECTIONS)

    chunk_peers_mapping = {}

    got_chunks = False

    async def _report_availability(avail_type: int):
        """
        Send a REPRT message to coordinator
        """
        assert avail_type in (0, 1, 2)

        reader, writer = await asyncio.open_connection("10.5.0.10", 8000)
        try:
            reprt_msg = pack(
                REPRT_body(
                    msg_type=MsgType.REPRT.value,
                    file_hash=str.encode(file_hash),
                    availability=avail_type,
                    file_size=size,
                )
            )
            writer.write(reprt_msg)
            await writer.drain()
        finally:
            writer.close()

    async def _report_partial_availability_task():
        """
        Periodically report partial availability if some chunks have been downloaded. Cancel this task before returning.
        """
        while True:
            if got_chunks:
                print("Reporting partial availability")
                await _report_availability(1)
            else:
                print("No chunks downloaded, skipping report")

            await asyncio.sleep(REPORTING_INTERVAL_SECONDS)

    async def build_chunk_peer_mapping() -> dict[int, list[str]]:
        """
        Query the coordinator for a given file, query peers with partial availability
         and map chunk numbers to peers which own them. Note that the mapping will not contain
         chunks which are not owned by any peer.
        """
        try:
            peers = await _query_coordinator_for_file_peers(file_hash)

            all_chunks_addrs = [peer.address for peer in peers if peer.availability == 2]
            partial_addrs = [peer.address for peer in peers if peer.availability == 1]

            # put all peers with full availability in mapping
            mapping = {i: all_chunks_addrs.copy() for i in range(num_chunks)}

            async def query_peer_for_partial_availability(addr: str) -> list[int]:
                """
                Send ACHNK to peer and get the list of available chunks from file.
                """
                reader, writer = await asyncio.open_connection(addr, 8000)
                try:
                    achnk_msg = pack(ACHNK_body(msg_type=MsgType.ACHNK.value, file_hash=str.encode(file_hash)))
                    writer.write(achnk_msg)
                    await writer.drain()
                    msg_type = await reader.readexactly(n=1)
                    if int.from_bytes(msg_type, byteorder="little", signed=False) == MsgType.ERROR.value:
                        body = await reader.readexactly(n=3)
                        error_msg = unpack(msg_type + body, msg_type=MsgType.ERROR)
                        print(error_msg)
                        return list()
                    prefix = msg_type + await reader.readexactly(n=39)
                    num_chunks = int.from_bytes(prefix[-4:-2], byteorder="little", signed=False)
                    chunks_bytes = await reader.readexactly(n=num_chunks * 4)
                    chunks_response = unpack(prefix + chunks_bytes, MsgType.CHNKS)
                    avail_chunks = [
                        int.from_bytes(chunks_response.availability[i : i + 4], byteorder="little", signed=False)
                        for i in range(0, num_chunks * 4, 4)
                    ]
                    return avail_chunks
                finally:
                    writer.close()

            # Map peers with partial availability to chunks they own
            partial_avail_addr_to_num_mapping = zip(
                partial_addrs,
                await asyncio.gather(*[query_peer_for_partial_availability(addr) for addr in partial_addrs]),
            )

            # add to mapping
            for addr, avail_chunks in partial_avail_addr_to_num_mapping:
                for chunk_num in avail_chunks:
                    try:
                        mapping[chunk_num].append(addr)
                    except IndexError:
                        mapping[chunk_num] = [addr]

            return mapping
        except ValueError as e:
            print(repr(e))
            return dict()

    async def refresh_chunk_availability():
        """
        Update `chunk_peers_mapping`.
        """
        nonlocal chunk_peers_mapping
        chunk_peers_mapping = await build_chunk_peer_mapping()

    await refresh_chunk_availability()

    # Put all unavailable chunks in retry queue
    for num in (num for num, addrs in chunk_peers_mapping.items() if len(addrs) == 0):
        await retry_queue.put((num, chunk_hashes[num]))

    def assign_chunks_to_peers(chunk_nums: list[int] = None) -> list[tuple[str, list[int]]]:
        """
        Randomly assign lists of chunks to peers. Put the rarest chunks at the start.
        """
        _chunk_addr_picks = [
            (num, random.choice(addrs))
            for num, addrs in sorted(chunk_peers_mapping.items(), key=lambda x: x[1])
            if len(addrs) > 0 and ((num in chunk_nums) if chunk_nums else True)
        ]
        _num_peers_per_chunk = {
            num: len(addrs) for num, addrs in sorted(chunk_peers_mapping.items(), key=lambda x: x[1]) if len(addrs) > 0
        }

        _addr_to_chunks_mapping = itertools.groupby(sorted(_chunk_addr_picks, key=lambda x: x[1]), key=lambda x: x[1])
        return [
            (addr, [_num for _num, _addr in list(sorted(grouped_list, key=lambda x: _num_peers_per_chunk[x[0]]))])
            for addr, grouped_list in _addr_to_chunks_mapping
        ]

    addr_chunks_pairs = assign_chunks_to_peers()

    async def _download_and_save_chunk(
        chunk_num: int, chunk_hash: str, reader: asyncio.StreamReader, writer: asyncio.StreamWriter
    ):
        """
        Download the given chunk and save it to `.partial` file
        """
        try:
            res = await _download_chunk(reader, writer, chunk_hash, chunk_num)
            if sha256(res).hexdigest()[:32] != chunk_hash:
                raise ValueError(f"Chunk {chunk_num} hash not matching")
            with open(partial_file, mode="r+b") as fp:
                fp.seek(chunk_num * CHUNK_SIZE)
                fp.write(res)
            nonlocal got_chunks
            got_chunks = True
        except Exception as e:
            print(f"{e} on chunk {chunk_num}")
            await retry_queue.put((chunk_num, chunk_hash))

    async def connect_and_save_chunks(host: str, port: int, chunk_nums: list[int], chunk_hashes: list[str]):
        """
        Connect to peer and download & save chunks sequentially
        """
        print(f"Connect to {host}:{port} and download {chunk_nums}")
        async with sem:
            reader, writer = await asyncio.open_connection(host, port)
            try:
                for coro in [
                    _download_and_save_chunk(chunk_num, chunk_hash, reader, writer)
                    for chunk_num, chunk_hash in zip(chunk_nums, chunk_hashes)
                ]:
                    await coro
            finally:
                writer.close()

    partial_availability_reporting_task = asyncio.create_task(_report_partial_availability_task())

    try:
        # Download and save all instantly available chunks
        await asyncio.gather(
            *[
                connect_and_save_chunks(addr, 8000, chunk_num_list, [chunk_hashes[n] for n in chunk_num_list])
                for addr, chunk_num_list, in addr_chunks_pairs
            ]
        )

        while not retry_queue.empty():
            # refresh availability
            await refresh_chunk_availability()

            items = []
            while not retry_queue.empty():
                items.append(await retry_queue.get())

            # find available and unavailable chunks
            available_items = [
                (chunk_num, chunk_hash) for chunk_num, chunk_hash in items if chunk_peers_mapping.get(chunk_num)
            ]
            unavailable_items = [
                (chunk_num, chunk_hash) for chunk_num, chunk_hash in items if not chunk_peers_mapping.get(chunk_num)
            ]

            addr_chunks_pairs = assign_chunks_to_peers([num for num, _ in available_items])

            # download previously unavailable chunks
            await asyncio.gather(
                *[
                    connect_and_save_chunks(addr, 8000, chunk_num_list, [chunk_hashes[n] for n in chunk_num_list])
                    for addr, chunk_num_list, in addr_chunks_pairs
                ]
            )

            # put chunks which are still unavailable back in queue
            if unavailable_items:
                for item in unavailable_items:
                    await retry_queue.put(item)

                print(f"Chunks {[num for num, _ in unavailable_items]} not available, retrying in {RETRY_SECONDS}s")
                await asyncio.sleep(RETRY_SECONDS)

        # remove null padding from file
        with open(partial_file, mode="rb") as fsource:
            with open(out_name, mode="wb") as fdest:
                shutil.copyfileobj(fsource, fdest)
                fdest.seek(-(CHUNK_SIZE - (size % CHUNK_SIZE)), os.SEEK_END)
                fdest.truncate()

        # check file hash
        with open("out.jpg", mode="rb") as fp:
            out_bytes = fp.read()

        if not sha256(out_bytes).hexdigest()[:32] == file_hash:
            print(f"File hash vs expected:\n{sha256(out_bytes).hexdigest()[:32]}\n{file_hash}")
            raise ValueError(f"File hash vs expected:\n{sha256(out_bytes).hexdigest()[:32]}\n{file_hash}")

        # report full availability
        await _report_availability(2)

        print(f"Hash is OK, wrote to {out_name}")
    finally:
        # cancel the availability reporting task
        if not partial_availability_reporting_task.cancelled():
            partial_availability_reporting_task.cancel()
        os.remove(partial_file)


def run_in_new_loop(loop, coro):
    asyncio.set_event_loop(loop)
    loop.run_until_complete(coro)
    loop.close()


def main_menu(user_input):
    try:
        command, params = user_input.split(" ", 1)
    except ValueError:
        command = user_input
        params = None
    match command:
        case "download":
            if params is None:
                print("No file name provided")
                main_menu("help download")
            else:
                print(f"Downloading file{params}")
                asyncio.run(download_file(f"{params[0]}.fileinfo", params[1]))
                # loop.run_until_complete(download_file(f"{params[0]}.fileinfo", params[1]))
        case "help":
            help(params)
        case "exit":
            print("Exiting...")
            exit()
        case _:
            print("Unknown command")


def help(params=None):
    if params is not None:
        print("Help for command:", params)
        match params:
            case "download":
                print("download <file_hash> <output_name> <source_port> <source_ip> - download file form peer")
            case "list":
                print("list <file_name> - list files containing <file_name>")
            case "help":
                print("help <command> - get help for command")
            case "exit":
                print("exit - exit program")
            case _:
                print("Unknown command")
    else:
        print("List of commands:")
        print("download")
        print("list")
        print("help")
        print("exit")


def main(port):
    # report_loop = asyncio.new_event_loop()
    # report_thread = threading.Thread(target=run_in_new_loop, args=(report_loop, automatic_reporting()))
    # report_thread.start()

    # server_loop = asyncio.new_event_loop()
    # server_thread = threading.Thread(target=run_in_new_loop, args=(server_loop, run_server(port=port)))
    # server_thread.start()

    try:
        while True:
            user_input = input("?: ")
            print(f">: {user_input}")
            main_menu(user_input=user_input)

    except KeyboardInterrupt:
        pass

    # if report_thread.is_alive():
    #     report_thread.join()

    # if server_thread.is_alive():
    #     server_thread.join()


def get_file_info(file_name):
    FILE = f"source.jpg"

    with open(f"{FILE}.fileinfo", mode="r") as fp:
        fileinfo = json.load(fp)
        return fileinfo


if __name__ == "__main__":
    global IP_ADDR
    IP_ADDR = sys.argv[1] if len(sys.argv) > 1 else "127.0.0.1"

    time.sleep(3)
    loop = asyncio.get_event_loop()

    fileinfo = get_file_info("source.jpg")

    loop.run_until_complete(download_file(fileinfo, "out.jpg"))

    # halt execution to allow for inspecting the container
    time.sleep(9999999)
